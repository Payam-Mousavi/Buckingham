{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring dimensionless numbers and Neural Network Regression\n",
    "Author: Payam Mousavi  \n",
    "Last updated: March 6, 2024 \n",
    "\n",
    "Given the analytical equation (fitted empirically) for the Drag Coefficient $C_D$ provided in reference: https://pages.mtu.edu/~fmorriso/DataCorrelationForSphereDrag2016.pdf, we see that it's only a function of the Reynolds number, $Re = \\rho.U.D/\\mu$. Let's say that we did not know this ahead of time and wanted to perform experiments for a range of input parameters, namely, $\\rho$, $U$, $D$, and $\\mu$. Clearly, many combinations will be redundant since they correpond to the same $Re$. Consider the following models:  \n",
    "\n",
    "Given $\\rho$, $U$, $D$, and $\\mu$, consider the following three models:  \n",
    "\n",
    "\n",
    "<img src=\"./Figures/Model_0.png\" width=\"300\" height=\"200\" /> . \n",
    "<img src=\"./Figures/Model_I.png\" width=\"300\" height=\"200\" /> . \n",
    "<img src=\"./Figures/Model_II.png\" width=\"300\" height=\"200\" /> . \n",
    "\n",
    "To start, we are interested in the following questions:  \n",
    "1. Model_0, we fit just to verify that it's an easy regression problem. Note that, given the very large range of inputs spanning multiple orders of magnitute, this is not a trivial problem. So we might choose to limit the range. For example, we expect a linear function for low Re (i.e., Stokes flow), so we could ignore that range. \n",
    "2. Assuming we keep the number of trainable parameters approximately the same, is Model_I more data efficient than Model_II, if we randomly sample the 4 parameters?  Intuitively, we expect this to be true since we are biasing the structure of the neural network to discover the existence of an effective compression of the inputs (by $Re$).  \n",
    "3. After training Model_I, does the single node actually correspond to $Re$? Can the model 'discover' the non-dimensional parameters?  If not, can we somehow help the model to discover this?  \n",
    "4. (Optional) can we use the knowledge of the $Re$ to perform data augmentation thereby increasing the efficiency of the training? For example, given a combination of inputs, we could generate $n$ additional training samples corresponding to the same $Re$. This 'synthetic data' is expected to improve the efficiency. It is not clear whether this is useful in practice. Still interesting to explore.  \n",
    "\n",
    "\n",
    "What's on the docket:  \n",
    "* Verify that MSELoss from PyTorch is consistent with my manual calculation. **DONE**  \n",
    "* Implement Model_0 to take Re as input and output $C_D$.  \n",
    "* Implement Model_I to take the 4 parameters, bottleneck to Re, then output $C_D$. Is the Re node equal to the value of Re?  \n",
    "* Quantify data requirements for each model to get similar accuracy? Do we need a hyperparameter optimization for this?  \n",
    "* Move some sections out of the notebook for more flexibility:\n",
    "    * Move dataset creation and loading \n",
    "    * Move helper functions to utils \n",
    "    * Move models \n",
    "* Tricks to improve fitting: scale the 4 inputs so that they are roughly in the same range.  \n",
    "* Limit the range of interest to only include regions with interesting features.  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from models import Model_II, Model_0\n",
    "from datasets import *\n",
    "from train import train_model\n",
    "from predict import predict_model\n",
    "from eval import eval_model\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "import torch.nn.init as init\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "eps = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re_vec = np.logspace(2, 7, 1000)\n",
    "# rho, mu, D, U = generate_inputs_from_Re(Re_vec=Re_vec, u_range=[0.1, 1e3], rho_range=[1, 1000], mu_range=[1e-3, 1e3], D_range=[1e-3, 1e3])\n",
    "# # rho# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_CD(Re, CD, marker='o'):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.scatterplot(x=Re, y=CD, color='blue', marker=marker, alpha=0.5)\n",
    "    # plt.xscale('log')\n",
    "    # plt.yscale('log')\n",
    "    plt.xlabel('$Re$')\n",
    "    plt.ylabel('$C_D$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Re_vec = np.logspace(2, 7, 200)\n",
    "# Re_vec = np.linspace(1e1, 1e7, 1000)\n",
    "CDs, _ = run_experiments(Re_vec=Re_vec)\n",
    "\n",
    "Re_vec_rescaled = Re_vec/1e6\n",
    "plot_CD(Re_vec_rescaled, CDs, marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation \n",
    "NOTE: Only run the dataset for the model of interest. Don't run both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloader/dataset:\n",
    "# dataset = RandomDataset(num_samples=50000,\n",
    "#                         Re_range=[1e4, 1e8],\n",
    "#                         rho_range=[100, 3000],\n",
    "#                         mu_range=[0.3e-3, 0.1],\n",
    "#                         D_range=[0.05, 1],\n",
    "#                         U_range=[1, 10],\n",
    "#                         seed=1234)\n",
    "\n",
    "dataset = RandomDataset_scaled(num_samples=50000,\n",
    "                               parms_scaling=[1e3, 1e-2, 1, 1],\n",
    "                               Re_range=[1e4, 1e8],\n",
    "                               rho_range=[100, 3000],\n",
    "                               mu_range=[0.3e-3, 0.1],\n",
    "                               D_range=[0.05, 1],\n",
    "                               U_range=[1, 10],\n",
    "                               seed=1234)\n",
    "\n",
    "\n",
    "# Split the dataset into train, validation, and test sets:\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=3)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False, num_workers=3)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=3)\n",
    "\n",
    "# Extract Re values for test set:\n",
    "train_Re = extract_Re_values(dataset, train_dataset)\n",
    "val_Re = extract_Re_values(dataset, val_dataset)\n",
    "test_Re = extract_Re_values(dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(train_Re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloader/dataset:\n",
    "dataset = RandomDataset_Re(num_samples=50000,\n",
    "                           Re_range=[1e4, 1e7],\n",
    "                           re_scaling=1e6,\n",
    "                           seed=123)\n",
    "\n",
    "\n",
    "# Split the dataset into train, validation, and test sets:\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=0)\n",
    "\n",
    "# Extract Re values for test set (used for visualizing histograms):\n",
    "test_Re = []\n",
    "for re, _ in test_dataset:\n",
    "    test_Re.append(re.item())\n",
    "\n",
    "test_Re = np.array(test_Re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rho, mu, D, U, Re_vec, CD = sample_parameters(num_samples=1000, \n",
    "#                                               Re_range=[1e0, 3e7], \n",
    "#                                               rho_range=[50, 3000], \n",
    "#                                               mu_range=[0.3e-3, 5], \n",
    "#                                               D_range=[0.05, 10], \n",
    "#                                               U_range=[.001, 10])\n",
    "\n",
    "# plt.loglog(Re_vec, CD, 'o', alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_Re, bins=10, edgecolor='black')\n",
    "plt.xlabel('Reynolds Number (Re)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Reynolds Numbers in Test Dataset')\n",
    "plt.show()\n",
    "\n",
    "min(test_Re), max(test_Re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model_II:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the model:\n",
    "HIDDEN_DIMS = [32, 32, 16, 8, 4]\n",
    "MODEL_NAME = 'Model_II'\n",
    "MODEL = Model_II(input_dim=4, output_dim=1, hidden_dims=HIDDEN_DIMS).float()\n",
    "CRITERION = nn.MSELoss() # criterion = nn.L1Loss(), nn.SmoothL1Loss(), ...\n",
    "OPTIMIZER = Adam(MODEL.parameters(), lr=1e-3)\n",
    "WRITER = SummaryWriter('runs/' + MODEL_NAME)\n",
    "MODEL_SAVE_PATH = 'models/' + MODEL_NAME + '.pt'\n",
    "WRITER_PATH = 'runs/' + MODEL_NAME\n",
    "\n",
    "NUM_EPOCHS = 500\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in MODEL.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {num_trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model_0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the model:\n",
    "HIDDEN_DIMS = [64, 32, 16, 8, 4]\n",
    "MODEL_NAME = 'Model_0'\n",
    "MODEL = Model_0(input_dim=1, output_dim=1, hidden_dims=HIDDEN_DIMS).float()\n",
    "CRITERION = nn.MSELoss() # criterion = nn.L1Loss(), nn.SmoothL1Loss(), ...\n",
    "OPTIMIZER = Adam(MODEL.parameters(), lr=1e-3)\n",
    "WRITER = SummaryWriter('runs/' + MODEL_NAME)\n",
    "MODEL_SAVE_PATH = 'models/' + MODEL_NAME + '.pt'\n",
    "WRITER_PATH = 'runs/' + MODEL_NAME\n",
    "\n",
    "NUM_EPOCHS = 500\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in MODEL.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {num_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_state, history = train_model(model=MODEL, \n",
    "                                        train_loader=train_loader, \n",
    "                                        val_loader=val_loader, \n",
    "                                        test_loader=test_loader, \n",
    "                                        optimizer=OPTIMIZER, \n",
    "                                        criterion=CRITERION, \n",
    "                                        num_epochs=NUM_EPOCHS, \n",
    "                                        model_save_path=MODEL_SAVE_PATH,\n",
    "                                        writer_path=WRITER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction:\n",
    "DATASET_TEST = test_dataset\n",
    "MODEL_SAVE_PATH = 'models/' + MODEL_NAME + '.pt'\n",
    "predictions = predict_model(model_path=MODEL_SAVE_PATH, \n",
    "                            dataset=DATASET_TEST)\n",
    "\n",
    "# Evaluation:\n",
    "mse, mae = eval_model(DATASET_TEST, MODEL_SAVE_PATH, visualize=True)\n",
    "print(f\"MSE: {mse:.5f}, MAE: {mae:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference & Testing:  \n",
    "Visualize the model fit by looking at the curves for a regularly-spaced Re vector covering the entire range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_cd_from_re(re_vector, model_path, hidden_dims = HIDDEN_DIMS, inputs=None):\n",
    "    # HIDDEN_DIMS = [256, 256, 64, 32] #TODO: add this as a parameter to this function\n",
    "    model = Model_II(input_dim=4, output_dim=1, hidden_dims=[256, 128, 64, 32, 32, 32, 16, 8, 4])\n",
    "    model_checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(model_checkpoint)\n",
    "    model.eval()\n",
    "\n",
    "    # Check if inputs are provided or need to be generated\n",
    "    if inputs is None:\n",
    "        # Generate inputs from Re vector\n",
    "        rho, mu, D, U = generate_inputs_from_Re(Re_vec=re_vector,  # Corrected variable name\n",
    "                                                u_range=[0.01, 20],\n",
    "                                                rho_range=[10, 2000],\n",
    "                                                mu_range=[1e-3, 0.1],\n",
    "                                                D_range=[0.05, 0.5])\n",
    "        input_tensor = torch.tensor(np.column_stack((rho, mu, D, U)), dtype=torch.float32)\n",
    "    else:\n",
    "        test_inputs = []\n",
    "        for idx in range(len(inputs)):\n",
    "            input_sample, _ = inputs[idx]  # Get the input sample and ignore the target\n",
    "            test_inputs.append(input_sample)  # Convert to numpy if it's a tensor\n",
    "\n",
    "        # Convert list to numpy array and then to tensor\n",
    "        input_tensor = torch.tensor(np.stack(test_inputs), dtype=torch.float32)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        cd_predicted = model(input_tensor).squeeze().numpy()\n",
    "\n",
    "    return cd_predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method #1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho, mu, D, U, Re_vec, CD = sample_parameters_naive(num_samples=5000,\n",
    "                                              Re_range=None,\n",
    "                                              rho_range=[10, 2000],\n",
    "                                              mu_range=[0.001, 0.1],\n",
    "                                              D_range=[0.05, 0.5],\n",
    "                                              U_range=[0.01, 20])\n",
    "\n",
    "cd_predicted = infer_cd_from_re(re_vector=Re_vec, model_path='models/Model-II-checkpoint.pth')\n",
    "cd_true = CD\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(Re_vec, cd_predicted, color='red',\n",
    "            label='CD Predicted', marker='x')\n",
    "plt.scatter(Re_vec, cd_true, color='blue', label='CD True', marker='o')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "\n",
    "plt.xlabel('Reynolds Number')\n",
    "plt.ylabel('Drag Coefficient (CD)')\n",
    "plt.title('Comparison of True and Predicted CD values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mse_error = np.mean((cd_true - cd_predicted)**2)\n",
    "print(\"MSE:\", mse_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Re_vec[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method #2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing:\n",
    "# Generate a random Re vector:\n",
    "Re_vec = np.array(train_Re) #np.logspace(2, 7, 200)\n",
    "# Re_vec = np.logspace(-1, 11, 5000)\n",
    "# Infer the CD from the Re vector:\n",
    "cd_predicted = infer_cd_from_re(Re_vec, model_path='models/Model-II-checkpoint.pth', inputs=train_dataset)\n",
    "cd_true, _ = run_experiments(Re_vec=Re_vec)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting predicted CD values\n",
    "plt.scatter(Re_vec, cd_predicted, color='red', label='CD Predicted', marker='x', alpha=0.5)\n",
    "plt.scatter(Re_vec, cd_true, color='blue', label='CD True', marker='o', alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Reynolds Number')\n",
    "plt.ylabel('Drag Coefficient (CD)')\n",
    "plt.title('Comparison of True and Predicted CD values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "mse_error = np.mean((cd_true - cd_predicted)**2)\n",
    "print(\"MSE:\", mse_error)\n",
    "r2 = 1 - np.sum((cd_true - cd_predicted)**2) / np.sum((cd_true - np.mean(cd_true))**2)\n",
    "print(\"R2:\", r2)\n",
    "\n",
    "\n",
    "plt.hist(Re_vec, bins=10, edgecolor='black')\n",
    "plt.xlabel('Reynolds Number (Re)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Reynolds Numbers in Test Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(Re_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
