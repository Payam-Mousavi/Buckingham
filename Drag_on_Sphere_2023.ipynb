{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring dimensionless numbers and Neural Network Regression\n",
    "Author: Payam Mousavi  \n",
    "Last updated: December 27, 2023 \n",
    "\n",
    "Given the analytical equation (fitted empirically) for the Drag Coefficient $C_D$ provided in reference: https://pages.mtu.edu/~fmorriso/DataCorrelationForSphereDrag2016.pdf, we see that it's only a function of the Reynolds number, $Re = \\rho.U.D/\\mu$. Let's say that we did not know this ahead of time and wanted to perform experiments for a range of input parameters, namely, $\\rho$, $U$, $D$, and $\\mu$. Clearly, many combinations will be redundant since they correpond to the same $Re$. Consider the following models:  \n",
    "\n",
    "Given $\\rho$, $U$, $D$, and $\\mu$, consider the following two models:  \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./Figures/Model_I.png\" width=\"300\" height=\"200\" /> . \n",
    "<img src=\"./Figures/Model_II.png\" width=\"300\" height=\"200\" /> . \n",
    "\n",
    "We are interested in the following questions:  \n",
    "1. Assuming we keep the number of trainable parameters approximately the same, is Model_I more data efficient than Model_II, if we randomly sample the 4 parameters?  Intuitively, we expect this to be true since we are biasing the structure of the neural network to discover the existence of an effective compression of the inputs (by $Re$).  \n",
    "2. After training Model_I, does the single node actually correspond to $Re$? Can the model 'discover' the non-dimensional parameters?  If not, can we somehow help the model to discover this?  \n",
    "3. (Optional) can we use the knowledge of the $Re$ to perform data augmentation thereby increasing the efficiency of the training? For example, given a combination of inputs, we could generate $n$ additional training samples corresponding to the same $Re$. This 'synthetic data' is expected to improve the efficiency. It is not clear whether this is useful in practice. Still interesting to explore.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "eps = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CD_calc(Re=None, U=None, D=None, rho=None, mu=None):\n",
    "    \"\"\"\n",
    "    Calculates the the Drag coefficient (CD) and the corresponding Drag force (FD), using the empirical equation found in the\n",
    "    reference: https://pages.mtu.edu/~fmorriso/DataCorrelationForSphereDrag2016.pdf\n",
    "\n",
    "    Use SI units\n",
    "\n",
    "    \"\"\"\n",
    "    if Re is None:\n",
    "        Re = rho*U*D/mu\n",
    "\n",
    "    CD = (24/Re) \\\n",
    "        + ((2.6*(Re/5.)) / (1+(Re/5.)**1.52)) \\\n",
    "        + ((0.411*(Re/2.63e5)**-7.94) / (1 + (Re/2.63e5)**-8.)) \\\n",
    "        + ((0.25*(Re/1e6)) / (1 + (Re/1e6)))\n",
    "\n",
    "    # Calculate FD if U and D and rho are given:\n",
    "    if U is not None:\n",
    "        FD = CD * (rho*np.pi/8) * (U**2) * (D**2)\n",
    "    else:\n",
    "        FD = None\n",
    "\n",
    "    return CD, FD\n",
    "\n",
    "\n",
    "def run_experiments(Re_vec, velocities=None, diameters=None, densities=None, viscosities=None):\n",
    "    \"\"\"\n",
    "    Runs the experiment by iterating over the velocity vector or Re_vec and returning the drag coefficient CD and Drag force FD\n",
    "\n",
    "    \"\"\"\n",
    "    if Re_vec is not None:\n",
    "        CDs, FDs = CD_calc(Re=Re_vec, rho=None, mu=None, U=None, D=None)\n",
    "    else:\n",
    "        CDs, FDs = CD_calc(Re=None, U=velocities, D=diameters, rho=densities, mu=viscosities)\n",
    "\n",
    "    return CDs, FDs\n",
    "\n",
    "\n",
    "def plot_CD(Re, CD, marker='o'):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.scatterplot(x=Re, y=CD, color='blue', marker=marker, alpha=0.5)\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlim(1e-1, 1e11)\n",
    "    plt.ylim(1e-2, 1e3)\n",
    "    plt.xlabel('$Re$')\n",
    "    plt.ylabel('$C_D$')\n",
    "\n",
    "    return True\n",
    "\n",
    "def generate_inputs_from_Re(Re_vec, u_range, rho_range, mu_range, D_range):\n",
    "    \"\"\"\n",
    "    Randomly sample from rho_range, mu_range, and D_range, calculate u, if u is in u_range, \n",
    "    output the parameters, u, rho, mu, D and if not, sample again and repeat. This is used to test inference.\n",
    "    \"\"\"\n",
    "    # Generate random samples from the ranges\n",
    "    rho = np.random.choice(rho_range)\n",
    "    mu = np.random.choice(mu_range)\n",
    "    D = np.random.choice(D_range)\n",
    "    U = Re_vec * mu / (rho * D)\n",
    "\n",
    "    for u in U:\n",
    "        if u >= u_range[1] and u <= u_range[0]:\n",
    "            print(f\"U = {u} is NOT in range\")\n",
    "\n",
    "    return rho, mu, D, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Re_vec = np.logspace(-1, 9, 1000)\n",
    "rho, mu, D, U = generate_inputs_from_Re(Re_vec=Re_vec, u_range=[0.1, 1e3], rho_range=[1, 1000], mu_range=[1e-3, 1e3], D_range=[1e-3, 1e3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAIlCAYAAAApNtk+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABBoUlEQVR4nO3de3TU9Z3/8dckw+RCEjBALkRIQIWQcjFCNhGI1ChoRZG62q2Ku2qr3Z7dssfdpnuq7KH2uIUtuLoWaN2z+ON3WP1pqa0GWtuCoBFI08hFY0IUL8EouYAICSEkTDK/Pz6d3AOZZCbfme88H+dwvsl3hplPPqDz4v25OTwej0cAAAA2EWF1AwAAAPyJcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGwlrMLN+vXrtXTpUt166616/fXXrW4OAAAIAKfVDRgp+/fv15EjR7R9+3adPn1at9xyixYsWKDo6GirmwYAAPwobMLN/Pnz9Vd/9VeKiIjQyZMnFRUVpcjISKubBQAA/CyshqWcTqfWrFmjO+64Q3fddZdGjRpldZMAAICfOcLxVPDTp09rxYoV+rd/+zfl5uZa3RwAAOBHYVO5+eijj/T+++9LksaOHav8/Hx98MEHFrcKAAD4W9iEm2PHjumJJ56Q2+3W2bNntW/fPmVnZ1vdLAAA4GdhM6G4oKBAhw4d0u23366IiAitWLFCM2fOtLpZAADAz0Jyzs2mTZtUUlKirVu3dt7r6OjQhg0btG3bNjU2Nmru3LlavXq10tPT/f7+hw4dksfjYUIyAAA+unDhghwOR0BHT0KucrNlyxY988wzysnJ6XF/06ZNevHFF7VmzRolJydr3bp1euihh7Rjxw65XC6/tsHj8cjj8aitrc2vrwsAAIYvZMJNfX29HnvsMR04cEBTpkzp8VhbW5uee+45FRYWatGiRZKkp556Svn5+dq5c6eWLl3q17aMGjVKbW1tysjIUExMjF9f285aWlpUXV1Nv/mAPhsa+s139NnQ0G++O3r0qCIiAjvlN2TCTUVFhcaMGaOioiJt3LhRn3/+eedjVVVVam5uVl5eXue9hIQEZWVlqayszO/hxismJkaxsbEBeW07o998R58NDf3mO/psaOi3wXM4HAF/j5AJNwUFBSooKOj3sbq6OklSampqj/tJSUmqra0NeNsAAEDwsMVS8JaWFknqM7cmKipKra2tVjQJAABYxBbhxnv4Ze8Jvq2trYyBAgAQZmwRbrzDUQ0NDT3uNzQ0KCUlxYomAQAAi9gi3GRmZiouLk6lpaWd9xobG1VZWal58+ZZ2DIAADDSQmZC8cW4XC6tWLFC69evV2JiotLS0rRu3TqlpKRo8eLFVjcPAACMIFuEG0lauXKl3G63Vq1apfPnzysnJ0ebN2/2+wZ+AAAguIVkuFm7dm2fe5GRkSosLFRhYaEFLQIAAMHCFnNuAAAAvAg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVgg3AADAVpxWN2Ck/fSnP1VxcbE8Ho/uuusu3X///VY3CQAA+FFYhZvdu3frgw8+0KuvvqrW1lbdeeedmj9/vqZNm2Z10wAAgJ+E1bDUxIkT9cgjjygyMlKxsbGaPHmy6urqrG4WAADwo7Cq3GRmZnZ+/c4776iyslLXXHONhS0CAAD+FlaVG6/Dhw/rH//xH7VmzRrFxcVZ3RwAAOBHYRdu9u7dq3/4h3/QT3/6Uy1YsMDq5gAAAD8Lq2GpY8eO6Qc/+IGeffZZzZo1y+rmAACAAAircLN582ZduHBBq1at6rz3/e9/X/n5+Ra2CgAA+FPIhptNmzappKREW7du7bzX0dGhDRs2aNu2bWpsbNTcuXO1evVqpaenS5J+/OMf68c//rHf2tDS0uK31woH3v6i3waPPhsa+s139NnQ0G++83g8cjgcAX0Ph8fj8QT0HQJgy5YtWrt2rXJycnqEmw0bNuiFF17QmjVrlJycrHXr1qmmpkY7duyQy+Xy2/uXl5erra3Nb68HAEA4cblcAZ0eElKVm/r6ej322GM6cOCApkyZ0uOxtrY2PffccyosLNSiRYskSU899ZTy8/O1c+dOLV261O/tycjIUExMjN9f165aWlpUXV1Nv/mAPhsa+s139NnQ0G++O3r0aMDfI6TCTUVFhcaMGaOioiJt3LhRn3/+eedjVVVVam5uVl5eXue9hIQEZWVlqaysLCDhJiYmRrGxsX5/Xbuj33xHnw0N/eY7+mxo6LfBC/SQlBRi4aagoEAFBQX9PubdaTg1NbXH/aSkJNXW1ga8bQAAIDjYZp8b72Su3nNroqKi1NraakWTAACABWwTbqKjoyWpz0Tf1tZWxkEBAAgjtgk33uGohoaGHvcbGhqUkpJiRZMAAIAFbBNuMjMzFRcXp9LS0s57jY2Nqqys1Lx58yxsGQAAGEkhNaH4Ylwul1asWKH169crMTFRaWlpWrdunVJSUrR48WKrmwcAAEaIbcKNJK1cuVJut1urVq3S+fPnlZOTo82bN/t1Az8AABDcQjbcrF27ts+9yMhIFRYWqrCw0IIWAQCAYGCbOTcAAAAS4QYAANgM4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANgK4QYAANiK0+oGwH+amqRjx6SzZ6W4OCk9XYqPt7pVAACMLMKNTVRXS0VF0qlTXfcSE6Vly6SMDKtaBQDAyGNYygaamvoGG8l8X1RkHgcAIFwQbmzg2LG+wcbr1CnzOAAA4YJhKRs4e7br68hIacIEyeORzp+XoqOllhbr2gYAwEgj3NhAXJy5RkZKEydKr78uHT/e9XhtrQk8zL0BAIQDhqVsID3dTB6eMKFvsImJkS5cYO4NACB8EG5sID7erIqKiuobbKZPN/eZewMACBcMS9lERoZ0xRVSVpbU1ia5XNKYMSbYeHWfmwMAgF0RbmwkMVFKSjJf9zexePRoa9sHAMBIINzYiHfuzZkzfScWx8RIzc1mCIuJxQAAO2POjY14595Mndo32EyfboalmFgMALA7Kjc2k5FhJg/v2yfFxvade+OdWDxzpqXNBAAgYAg3NtTWdvG5N2zqBwCwM8KNDbGpHwAgnDHnxobY1A8AEM4INzbEpn4AgHDGsJRN9d7ULyZGmjZNcjqZewMAsDfCjY15N/Vj7g0AIJwwLGVjzL0BAIQjwo2NMfcGABCOGJayOebeAADCDeEmDDD3BgAQThiWCgPMvQEAhBPCTRhg7g0AIJwwLBUmes+96X2gpmRODQcAINQRbsKId+6N1P+BmqNHW9s+AAD8gXATRrxzb86c6TuxOCZGam42Q1hMLAYAhDLm3IQR79ybqVP7Bpvp082wFBOLAQChjspNmMnIMJOH9+2TYmP7zr3xTiyeOdPSZgIAMGSEmzDU1nbxuTds6gcACGVhG27q6+t19913a/fu3VY3ZcTFxZkrm/oBAOwoLOfclJSU6O/+7u904sQJq5tiCTb1AwDYWViGm5dffllPP/201c2wTH+b+jmdUna2tGSJGbIaNUqqrra0mQAADElYDkutX7/e6iZYrvumfu3t5rpvn3ToUNdzPv1U+s53GJ4CAISWsKzcwPBu6pedLZWWSg0NPR8/c4bhKQBA6CHchDHv3BuPp+e8G8nMvRkzhjOnAAChJyyHpWB459689lrXPadTmjVLmjzZfM/ScABAqCHchLmMDCk3VyovH3juDUvDAQChJKTDzaZNm1RSUqKtW7d23uvo6NCGDRu0bds2NTY2au7cuVq9erXS09P7/P7y8vJhvX+LTUoakyZF6MorI+RyReittxyqre16LDZWamnx6Ne/7tB993Vo9OiOIb+Pt7/s0m8jgT4bGvrNd/TZ0NBvvvN4PHI4HAF9D4fH4/EE9B0CZMuWLVq7dq1ycnJ6hJsNGzbohRde0Jo1a5ScnKx169appqZGO3bskMvl8st7l5eXq62tzS+vFQwcDodaWpJ0+PBYPfdc11+H0aMduuqqDjmdzfJ4PPrGNxy67LLjF3klAAAuzeVyadasWQF7/ZCr3NTX1+uxxx7TgQMHNGXKlB6PtbW16bnnnlNhYaEWLVokSXrqqaeUn5+vnTt3aunSpX5tS0ZGhmJiYvz6mlZqa3Np3jyHLlwwc22mTfPI6XTo/PnRiomR4uI8mjFjzJBfv6WlRdXV1bbrt0Ciz4aGfvMdfTY09Jvvjh49GvD3CLlwU1FRoTFjxqioqEgbN27U559/3vlYVVWVmpublZeX13kvISFBWVlZKisr83u4iYmJUWxsrF9f00oTJkipqQMfy1BfL6WkOIc998Zu/TYS6LOhod98R58NDf02eIEekpJCMNwUFBSooKCg38fq6uokSampqT3uJyUlqbb7RBL0y7s03OW6+LEMDzxgVloBABCMbLXPjXdCV++5NVFRUWptbbWiSSGlv2MZJBNspk8399n3BgAQ7EKucnMx0dHRkszcG+/XktTa2spY6CD1PpZh6lSzYsrtNvNwHA6pudnqVgIAMDBbhRvvcFRDQ4Mme3eh+8v3mZmZVjUr5CQmmrk3/c27mTjRHNcAAECwstWwVGZmpuLi4lRaWtp5r7GxUZWVlZo3b56FLQst6emmYtM72EjSl19Kb73FeVMAgOBlq8qNy+XSihUrtH79eiUmJiotLU3r1q1TSkqKFi9ebHXzQkZ8vBme+vJL873TKU2bJo0bZ4aoRo+WqqvNMQ0AAAQbW4UbSVq5cqXcbrdWrVql8+fPKycnR5s3b/bbBn7hIjJSuvpq6exZacoUqbhYevddE3Qk6dNPpe98hyMZAADBJ6TDzdq1a/vci4yMVGFhoQoLCy1okX3ExZnVUenpJticPt0VbCTpzBmWhQMAgpOt5tzAf7x73ng8fefdxMRIY8awLBwAEJxCunKDwPHuefPaa133nE4zz8a7EC06WuKsOABAsCHcYEAZGVJurlRebva8ycqS9u2TDh3qek5trTm2gbk3AIBgwbAULuqqq6TMTLO3TWmp1NDQ9Vj3IxlYGg4ACBaEG1xUf0cyOJ0m7CxZIiUlSaNGmaXhAAAEA4alcEm9j2Tob3iKpeEAgGBB5QaDkphoqjT9DU9JXUvDGZ4CAFiNyg0Gpb+l4d13Lk5I6BqeYudiAICVCDcYlN5Lw51Os5KqpEQ6d84sC5cYngIAWI9hKQyad2l4Vpb01a9KFRVSW1tXsJEYngIAWI/KDXziXRo+alTXkQwMTwEAggnhBj5heAoAEOwYloLPGJ4CAAQzKjcYEoanAADBinCDIWF4CgAQrBiWwpAxPAUACEZUbjAsvgxPXXGF1a0FAIQDwg2GxZfhqQcfdMrhcFjXWABAWGBYCsM22OGp7dslKd6aRgIAwgaVG/jFYIanoqIidfZsotVNBQDYHOEGfjGY4an2duno0Silpjp11VXWthcAYF8MS8FvBjM8deqUR9u3s3oKABA4VG7gVxcbnrrsMoeio52Kiopkcz8AQMAQbuBXFxueOntWam93KzbWpc8+Y3M/AEBgMCwFvxtoeCoqytP5HDb3AwAECpUbBER/w1MOhzR7dqQmTnRo7FjOngIABAbhBgHR3/BUTo5Db70VqYoKKTbW3OfsKQCAv/kl3Fy4cEH79+9XQ0ODLrvsMmVnZ2vcuHH+eGmEMO/wVHm5NHGiVFYmtbS0Kza266+dd3jqgQdMIAIAYLiGHW6++OIL3XvvvTpx4oTS0tJ09uxZ1dfX66abbtKqVauUmMimbeGs+/DUqVMeSe1yOqUZM/qePcXwFADAH4Y9oXjDhg2aNWuWSkpKVFRUpN27d2vXrl2Kjo7WPffcoyZmjIY17/CUl9Mp5eU5VFUl7dwp/fa30ksvSS+8YAIOAADDNexws3//fn3/+9+Xy+XqvJeamqqf/OQnys7O1nPPPTfct0CI6756avHiSFVW9n/2FKunAAD+MOxwc/LkSSUnJ/f72He+8x3t3r17uG8BG7jqKmn69HYlJLTr1ClP5+Z+WVlSfr40ZUrX8BQAAMMx7Dk3Ho9nwMcyMjJUV1c33LeADcTHS7fdJhUVmTzd39lTEqunAADDN+xwc+7cOd16662aPn1656/MzMzOao7b7R52I2EPaWluzZlzXh98EK1Jk6S33x54eIrVUwCAoRp2uHn99ddVVVWlqqoqvffee/rVr36lmpoaJSQkaNq0aWptbfVHO2ETiYmnNH16vGJiIvucPcXqKQCAP/gUbpqbm7V161bl5+frK1/5iiQpLS1NaWlpuuGGGzqfd+7cOb3//vuqqqrS1KlT/dtihLgm3XabtGuX+Y7hKQCAv/kUbp5//nn94he/0JIlSy76vNjYWGVnZys7O3tYjYP9eDwepaW5lZvr6tzcj+EpAIA/+bRaaufOnfrrv/7ri1ZjSkpKdN9996mqqmrYjYN9eTf3GzdOPYanWD0FABgun8LNhx9+qIULF170Oddee63Onz+v7du3D6thsLf+NvfLzRWb+wEAhs2ncNPR0aHRo0df8nm33HKLSktLh9wohIfum/t99atSRQWb+wEAhs+ncDNhwgR9+umnl3zetGnTVFtbO+RGIXwwPAUA8Defwk1OTo5eeeWVS79oRIQaGxuH2iaEEYanAAD+5lO4ufvuu3XgwAFt2rTpos/76KOPOA0cg8bwFADAn3wKN7Nnz9a3v/1tPfPMM3rkkUd07NixPs85deqUNm/erNzcXL81EvbH8BQAwF983qH4X/7lX+RyufSLX/xCf/jDHzR37lzNmDFDEyZM0PHjx/Xaa6+ppaVFDz/8cCDaC5vyDk+99pr5ns39AABDNaTjF773ve/p+uuv189//nMVFxerrKys87FJkybp6aef1pVXXum3RiI8eIen2NwPADAcQz5baubMmdq4caNaWlp09OhRNTU1afz48Zo+fbo/24cw4x2eGjVKnD0FABiSYR+cGRMTo9mzZ/ujLQDDUwCAYfNpQjEwElg9BQAYjmFXboBAYHgKADBUhBsEJYanAABDRbhB0Opv9VRHh3TNNaZy09YmjR8v7dtnqjmsngIASIQbBLnuw1Nnz0oLFkjFxVJNjanmpKRIkydLV15pghAAAEwoRlDrfvbUtGk9g8348eZ6/Lj0yitMLgYAGFRuEPS8w1PNzdKf/iRNmCCNHm1WU3UfnmJyMQBAItwgRFx1lVRaasKMd3Jx7+GpkyeZXAwAYFgKISI+Xpo9W4qJGXh4ir1vAAASlRuEkNmzpRtuMF/3Hp5i7xsAgBfhBiEjPl66806z90334Sn2vgEAdMewFELKQEczxMWZe3l55us33pDq6y1uLADAElRuEHJ6H80QHd01wbi21kwudjqljz+WHnyQCg4AhBsqNwg53fe+kbomGNfWdk0ulsz+N0wwBoDwQ7hBSOo+PJWRIZ0/byo23YenpkzpmmAMAAgfDEshZHmHp9rbzaqp7vvfdB+eYoIxAIQXKjcIWd7hqbFjzfcDDU+x/w0AhBcqNwhpGRnSLbdIn3xihqTKykzFJjrahB32vwGA8EO4QchLTjarol57refwFPvfAEB4YlgKtsD+NwAALyo3sA32vwEASFRuYCPsfwMAkAg3sBn2vwEAMCwF22H/GwAIb2FVufnNb36jW265RUuWLNHvfvc7q5uDALnY/jcpKdLs2UwwBgA7C5vKTX19vX7+85/r5ZdfVkdHh/7mb/5GeXl5SkxMtLppCID+9r+5/HJpwQImGAOA3YVN5Wb//v1asGCB4uPjNWbMGM2fP1979uyxulkIIO/+N/HxZngqK6v/Co7TSQUHAOwkbCo3DQ0NmjBhQuf348eP14kTJyxsEUaCd4Jxebn5uncFp6ZGmjDBhB8qOABgD2FTueno6JDD4ehxLyIibH78sNZ9/5vuFZyaGikmRpo7lwoOANhJ2FRuUlJS9N5773V+/8UXX2gWBw2FBe8EY+8oZEJCV7BZtkzav58KDgDYSdiULq699lrt3btXjY2Namxs1N69e5WXl2d1szBCvBOMr7tOio01QWbRoq5g43RKUVHmuWzyBwChLawqN9/97nd1zz336MKFC3rooYeUkpJidbMwgrwTjPfsMRWasWO7go13gnFCgqn0cIo4AISukAw3mzZtUklJibZu3dp5r6OjQxs2bNC2bdvU2NiouXPnavXq1UpPT+98zvLly7V8+XILWoxg0X2JuNPZNRTV3xJxNvkDgNAUcuFmy5YteuaZZ5STk9Pj/qZNm/Tiiy9qzZo1Sk5O1rp16/TQQw9px44dcrlcAWlLS0tLQF7Xrrz9ZXW/xcdL993nVHFxhKKjHZoxQ3rzTYdqa011Z9YsjxISHIqNlXbv7tDixe0aN67dkrYGS5+FGvrNd/TZ0NBvvvN4PH0W+Pibw+PxeAL6Dn5SX1+vxx57TAcOHFBKSorGjx/fWblpa2tTXl6eCgsLdffdd0uSGhsblZ+fr5/85CdaunSpX9tSXl6utrY2v74mRpbD4ZDDkaznnx+jsWNd2rbNo+joduXnR2jvXodqax0aP94tqV3XX+/Q179+WjExDQqR/1wAIKi5XK6ALuoJmcpNRUWFxowZo6KiIm3cuFGff/5552NVVVVqbm7uMUE4ISFBWVlZKisr83u48crIyFBMTExAXtuOWlpaVF1dHVT99p3vOPXHP0YoIcGhmTOd+tOfHDpxQkpLk+bMieys4Lz/frQWL54w4hWcYOyzUEC/+Y4+Gxr6zXdHjx4N+HuETLgpKChQQUFBv4/V1dVJklJTU3vcT0pKUm1tbcDaFBMTo9jY2IC9vl0FU79ddZVZFVVRYebWvP22NGlSf5v8Rai62mnZEvFg6rNQQr/5jj4bGvpt8AI9JCXZZCm4d6yz99yaqKgotba2WtEkhBA2+QMAewmZys3FREdHSzJzb7xfS1JraytlQlwSm/wBgL3YonLjHY5qaGjocb+hoYG9bDAol9rkjwoOAIQOW4SbzMxMxcXFqbS0tPNeY2OjKisrNW/ePAtbhlDi3eRv0qSem/x5KzgVFdLLL0vHjklVVdL//q+0bx87GQNAsLHFsJTL5dKKFSu0fv16JSYmKi0tTevWrVNKSooWL15sdfMQQvrb5G/uXFPBqauTbr5ZOnRIev11KTFR2rVLuuEG6c47GaYCgGBhi3AjSStXrpTb7daqVat0/vx55eTkaPPmzQHbwA/2NdAxDbm50oED5uypUaOk7GwTflpbzTDV175mfi8AwFohGW7Wrl3b515kZKQKCwtVWFhoQYtgN/1VcNLTpdJSE2x6V3ASEphoDADBwhZzboBA6D0Hx+0296+5xlRwPvusq4LDRGMACB4hWbkBRkr3Ck5c3MUrOCwVB4DgQLgBLsFbwdm3z2z417uCc+KEmY+Tnm4ei4szzx03zuyhAwAYWYQbYBAyMkxYufJK6c9/7qrgHDxoqjcHDpiv58yRHA7p7FmppES69loCDgCMNMINMEjx8aZCk5FhqjVOp9n079Ah8/2SJWbJ+J//LEVESLt3S8uXs0wcAEYaE4oBH3WfaDxhgplYPGeOCTYnTkhRUdK8eSYIeZeJM8kYAEYOlRtgCLwTjU+dMkvB09NNxSYmpquCc+iQqew0NUlffGFWVM2ezTAVAAQa4QYYouRkE1h27ZI8HjMU5a3gnD5tws/Bg9LevdLChebeoUNm6TghBwACh3ADDMPs2eb4BalnBeev/sqEnMZGM+H44EGz4/GECdJvf8uRDQAQSMy5AYYhPt6ElNRUKTOzq4KTnGzm32Rnm5Dz+edmX5yZM011h7k4ABA4VG6AYepvmfioUWY1VXKy9Kc/dc3F6T5MxVwcAAgMwg3gB72XiXt3M/aGHO9cnO7DVMzFAYDAYFgK8CPvMvG0NLObcXR01/3uw1QNDaaSU1UlvfiiCTe//730f/+v2d24qcnanwMAQhmVG8DPeg9TXX1132GqvLyelZy335aKi83S8fr6rkrOtGkRcjgcVv9IABBSCDdAAHiHqbKyzMqpgebi5OWZ4anTp6Vbb+0bcg4eHKWrrrpSkyZFKDbW6p8KAEID4QYIoEvNxZkwwRzTcP31/YechQsd+uyzaB09GqlrrmFODgAMBnNugBEw0Fyc9nYpNlZKSpJqasyxDXv3SnV1Zk5OZaX0//6fRwcPOpiTAwCDROUGGCH9zcUZO9ZUcLqHnD17uio5X37p0Ne+FqnDh833110nnTwpHTtmNg2Mje3aPJCKDgAYhBtgBPU3F+diIeerXzUTj8+ccei226TDh03V5w9/MENYS5aY0BQdbYasGLYCAMINYInBhxyPXn/doRtuMJWbKVO8FR3pttvM7/vjH6VFi8wEZCo6XRwOh5qbI/TJJ9K5c1JzszmxvbXVXM+flyIj6SfAjgg3gIUGE3JiYjxKSvLojTccuvZac2zDV7/aM+S8+675oPZWdL72NfM6Bw9Ko0ebUHTVVfb9AG9qkj79tCvEjBrlUlPTV7Rtm1MpKWZl2tSpUlmZWWJfVia1tUk5OdLkyVS+ALsh3ABBYKCQc9llksPRrvZ2sxTc7TZnV6Wk9Aw5GRldYWf5csnlkp56yuyjM2+eNH26CTdJSVJCQuhWLXqHmOho8/WuXV3L66dOlU6fjtDbb0vp6Q69+aY0aZIZ5ps40VS6GhvNqe1lZdL27aY/i4o40BSwC8INEER6h5zSUo+uucaEHKfTDKf0F3Jyc6U33zRh5/Rp80FeVyfdcYdZVj52rPTaa+Z5771nfn9OjvkQT042r5+QYN2QTe/Q0n34qLV14BBz5oypTk2aZPrBG2Jmz5aqqhy6+mqHdu0y3+/caa67dpl+Ki42fZWTY35Gj0dyOMxqtHHjQifwAeiLcAMEIW/ImTr1grKy2lRZ6dTVV5thqilT+oac9vausNPaKlVXm3Or9u+XLr+86/rmm6ZqsWyZ2QXZ7ZZ+8xtp1izpyBFp7tyeQzYDhZ+hXKOjzfs1NZnKkmSGzPoLLd7hI+9Ozc3N/YeYOXPMERbdw8vOnVJmpkPt7R1qa5M6Osz7e6+S2V9o3z4T/v70J/N1VJQJc5mZZkVbbq41f/YAho9wAwSx0aM7lJj4oe6+O0u5uZHavt0Mm9TX9ww50dFdYad70Nm7t2vvnHnzpLfeMkMvxcXm9/e+dh+yGSj8eAOHL9d580ybiotNIHnnHXN1u83XvUOLd/ho6lRzzc3tP8TMmNE3vHR0mKqTZKpd/V3dbtOm/fvN/kLe3Z8jI6Xjx6VXXjHVM6o3QGhiEz8gyHk8Ho0e3aHcXOlf/1W68UZTcXj8cTOXZt68nhWdmBgzvOINOd6r223uJyebVVUTJvS9fvKJCTLFxWYoq/s1Ls4Ejfh436/Hj5u5LXFxXa9VXGzCyJEjpjJUVdV1HT/etGXcOHPtHV4uFWK84a+hwQw5nTjRdfX2U2qqGQqLiDC/zzvsJ5nhrmPHRvpPGoC/EG6AEBIfL33lKybQFBRIDzwg/fCHJgzceac0Zoz5sJ4yxYScyMiusOM98qG93byW293/tb/Q0z38eAOHL1eHQ/r4YxOsul8vXDDv2194kczj0sAVmPr6vuElPd3sB5SfL50+7dF115m5NdddJ509a/YGcjrNMFtkpKnauFymD51O019jxpjnAghNDEsBIaz7BORPP5VaWsycljlzTHjIypK++MKEncZGM4zjcpkAFBXV9yoNHHq8V2/g8OXqdpsJu72vTmfXVep7HTXKXHuHl+4hZtkyqbzchBfv9d13PSora9eCBQ5NmSItWGCqQvfc0zX/5+TJrvlFUVFdwWb6dPN9XJxf/ogAWIBwA9iAt6LjNW+eqbYsXmz2vlmwwCwvv/56M+SSmWmWjXe/njzZNWRzsfDjDRy+Xh0OEyC6X+vqzHv2F16mTOkKZuXlZh5Q7xBTXm7m9HgnP3tDzN/8TYe+/PKC4uIiNWFC/yu+mprMENjHH5uA43KZio13YnF6ul//iACMIMINYEPx8dLMmeZXXp6p6ixc2LU3zB13mBVK8+eb1ULXX2+qGEuWmOdcLPx4A4cv1/h4Mzm4vr7n9eBB6a67zPL07qHlyBHTlkOHzLWsrP8Qc889/S9bP3euTUeOvK8ZM2YoNrb//83Fx5uhvKIiE/gmTDBVJMlUvgCELsINYHO9qzpeOTkm9MyfbwLNrbd2DdlcLPx4A4cv1+nTzVL04mITXt55p+t6+HD/oaW11WxI6L0GYu+djAwzb6my0qyQOnPGVG/q683+QMuWsaEfEIoIN0CYGij0eA0UfroHDl+u0dHSzTeb4aCvf91USQaqvIy00lIzHJWU1HXv1ClT1XngAZaEA6GGcAOgX5cKP3Zx7JgJMv05dco8PnPmyLYJwPCwFBxAWLvUkm+WhAOhh3ADIKxdask3S8KB0EO4ARDW0tPNfJ/+sCQcCE2EGwBhLT7erIrqHXASE6Xbb2cyMRCKmFAMIOx5l4QfO2bm2MTFWbdyC8DwEW4AQF0bHwIIfQxLAQAAW6FyAwDdNDUxPAWEOsINAPxFdbXZlbj7pn6JiRzDYKWmJrNT9rlzZqfsqKiuHa/dbvO4y2WeGxtrHvM+ZySu58655PFM09tvu9TcbG1bel+DYfdvqxBuAEDmQ7J3sJE4hmGwBgohQ71GR5vX2rVLSk42Z5xNnWrOOJs3z5xUX1xsDjl95x1zragwx4YcOiRlZ4/Mde7cCEkx2rs3QldfbW1bul/Lykz/XXaZOdtt6tTwCukOj8d7Di4Gq7y8XG1tbX85cTjW6uaEjHPnzunIkSP0mw/os6EZSr+99570q18N/Pidd9p7wvHF+qypyVS1amtNJUDqqkxcLIQM58O5udmcGj9pklRT03WdONEMFx46JKWlSZ9/3vNaW2ve/+OPR+YaH9+hAwc8uvzyCB0/7rC0Ld2vX3xhqkgpKSaUX321lJoaHCH93XfflcPh0KxZswL2HkwoBgBxDIMkORwJ+uADp8rKpDfeMEFlzx7p+eelxx83Xz/xhDlB/bHHpNdek/73f6VVq6SPPpJeeknq6DDPS0iQ/vhH80E6lKvbLVVVmcpD9+v48ZLDYT7Ak5P7Xj/8UBo3Tvrkk5G5OhwOffyxQykp1rel+9XtNr9aW6WWFnPivfestHBAuAEAhecxDE1NZvjk7bel/fuj9Pzzk7Rnj1P/+Z/Szp0muLz4ovTss1J7u/T730tjx5rhoOjoS4eQ4Xw4t7aaoNTf1e02p8oPdL1wwfx8I3Ht/rXVbel+9Y7JtLeba1ubuYZDSJeYcwMAkrqOYejvhHC7HMPgnRfT0iI1NnYNJdXXS8XFDqWkRGr/fmnyZFN9mTPHfFh+/LEZKtq921yrq6VrrpHeessMdwwUQqShfzg7nRe/Ohzm6/6uo0aZ54zU1ft1MLTFe3U4zNU7jOid6GzHkN4fKjcAIHsfw9DUJJWWSv/xHybQ/PrX0urVXUNJZ85In3zi0GWXOVRV5eisvrS1mbDR0dH36q1MXCqEDPXD+cQJEyj7u3o8Zm5JfX3f65VXmvkmU6aMzNXj8WjqVI/q6qxvS/er02l+RUVJMTHSmDH2CemDQbgBgL/wHsNw553SzTeb6wMPhN4HQvfhpt27pf/zf6Q1a6STJ82kaRNmzIddVZVZMnzunOR2O3pUXyIjzQekN8T0vjocFw8hw/lwLi+XrrtOOn265/XsWTOp+LbbzNfee9ddZ36OJUvMzz9S19RU6dZbPTp71mN5W7pfr7zSDA3Gx5vVUqmpoR/SfcFqqSFgtdTQsPLHd/TZ0IRrvzU1SZWV0vbt0oQJpnrw1lvS7NnSq6+aD7dXXzXB7de/lr7+denll6VvfEN66SWPvvENj375S4e+8Q2Htm2TFi0y/+I/dEi6/HLps8+6rpMmmdVAjY3SLbeYMDJrVtf1yBFp7tzhLWVuazPLqTMyzPDZZZeZicq997mJijKBy5p9btzyeFrkcsWoudlpaVtCZZ+bkVgtxZwbAAhx3lDz5z9Lr79ulv+++aYJNZ98Yv7lfu5cV4XG6eyqzEgmBGVkeFRX51FGhnTihEPp6dLhwyYA3XqrGc5auNBUgxYtMu+3ZIkJIYcOdYWQBQtMCLnnHvMhu3z50K/B+uHc3blzbTpy5IO/BGk+UoMFfxIAEKJ6h5pp00wgWbq0Z6jpHmY6OqS6OhMW6uvVGWJuuUV65x2P8vMdeu89M7RSXm7CTF6e9L3vmY3zbr/dzIu5997QCiEIL4QbAAgxA4WapKSuvU26h5reYebgQTNn5d13TYg5dMisjrr++nZdeWWEFi7sqr4QXBCKCDcAECIuFWoiI/sPNd3DzMKF5vvdu82w0tSpZigpIUFqbz+nyy8frSuvdBFiENIINwAQ5AYbahoazETf3qHGG2YWLTITjb/3PVOJiY3tqshERp7XkSMfa/r0GYqNdVn9IwPDQrgBgCBWXS3t2yf99rdmc72LhZq33zaTfw8f7luhycgwK4xmzza/eldmzp0b+Z8NCBTCDQD0o6nJnMNz9qzZ1dWK+Sb19dJzz5n3P3rUhBpp4FDz9tvmSITrrhu4QsNwE8IB4QYAeqmuloqKeh7FkJhodjDOyAj8+3cfhnrtNTMn5sQJszuw222+HijUTJxo9oTJzu6/QgOEA8INAHTT1NQ32Ejm+6Iis2NxIAND92GolJSeoaa+3lRkDh0yG/ERaoD+EW4AoJtjx/o/PFMy948dk2bODMx7NzWZ4xEkcyxCaqpZ9dQ71Bw8SKgBLoZwAwDdnD07vMeHqqlJKimRXnnFBJWTJ02QGjdOOnDAbLLnDTULF5pjEAg1QP8INwDQTVzc8B4fCu9Q1Pvvm3ObZs0yIcobavbvl37/e1OpSU42E4tzcwk1wEAINwDQTXq6mTzc39CUd8WRP3VfEdXcbIahvBOGa2pMqMnO7mrXxIkm8CQn+7cdgJ1EWN0AAAgm8fFmVVRiYs/7iYnmXCV/Vkqqq6Xf/U4qLjYb73Ufhlq40ASclhYzcbi01Jz0XFBAsAEuhcoNAPSSkWFWRQVynxtvxcbpNBUbp1N65x2z4Z53GCo7W7rhBvP+CxdK117LMBQwGIQbAOhHfHzgVkVVV5uDKouLzYnbJ06YIDVhgpkwPGeOCVMejxmGSk0l2AC+INwAwAjqXbE5fdoMPx08aJZ5HzpkhqE+/tgMP02caKo2BBtg8Ag3ADBC+qvYvPmmmePTfUXUwoWsiAKGg3ADACNgoIpNTY3Z+XjRInPMQmysuc+KKGDoWC0FAAHWfVXUuXNdFZv587tWRB04IP3pT+aYBVZEAcND5QYALmK4p4N7z6pqbzffNzZSsQECjXADAAPwx+ngR4+ac6IyMsxwVGWlmVdTXGwCzoED0ujR5h4VG8A/CDcA0A9/nA5eXW0236usNN9HR5vjFd5800wUpmIDBEbYzrmpr69XQUGB1c0AEKQGczr4xXgnEDc19azYpKaa3YjffZc5NkCghGXlpqSkRI8//rhOnDhhdVMABKnhnA7efcl3Vlb/FZv4eM6JAgIlLCs3L7/8sp5++mmrmwEgiA31dHBvxaam5uIVm7NnqdgAgRKWlZv169db3QQAQW4op4P3t0nfl1/2rdgkJpqN+vx9wjgAw7bhpqioqE915sYbb9Sjjz5qTYMAhBTv6eD9rZbq73Twiy35rqsz95xO6corze8d7GorAL6zbbhZtmyZli1bZnUzAIQwX04Hv9SS79ZWE24mTuw/HAHwH9uGGwDwB+/p4N7N/Coq+oYclnwDwYVwAwCXcLHN/GJizATiuLi+FZvaWvNcp5NN+oCRFBLhZtOmTSopKdHWrVs773V0dGjDhg3atm2bGhsbNXfuXK1evVrpPszQKy8vH1a7WlpahvX7w423v+i3waPPhsaf/dbcHKFf/zpCX3zh6HH/5Elp716HWloc2rPHVHeiohyqqTGTiufM8Wj+fIfi4qSJEzu0ZEm7xo1r17lzw25SQPB3bWjoN995PB45HI5LP3EYHB6PxxPQdximLVu2aO3atcrJyekRbjZs2KAXXnhBa9asUXJystatW6eamhrt2LFDLpcroG0qLy9XW1tbQN8DQHD48suJ+uUvzf8mIyOlSZOi5HK5NHnyKO3a5VRSkkPbt7crOtqt/PwI7d3rUG2tQ+PHuyW1Kz/foW9847SioxsU5P+7BUaMy+XSrFmzAvb6QVu5qa+v12OPPaYDBw5oypQpPR5ra2vTc889p8LCQi1atEiS9NRTTyk/P187d+7U0qVLR6SNGRkZiomJGZH3soOWlhZVV1fTbz6gz4bGn/124MAoTZhggs3llzv0xhsOpac71NAgvfSSQ9/8pnTyZISio53av1/KzvZo0SKH4uKcuuwyjxYu7NC0aYmSEv3zwwUIf9eGhn7z3dGjRwP+HkEbbioqKjRmzBgVFRVp48aN+vzzzzsfq6qqUnNzs/Ly8jrvJSQkKCsrS2VlZSMWbmJiYhQbGzsi72Un9Jvv6LOh8Ue/XXaZNGqUlJIivfGGNHastHevNH265HCYTfuSkqQvvjCBp7y8a8n3mDHSFVdEKjZ2lF9+npHA37Whod8GL9BDUlIQ71BcUFCgJ598UpMmTerzWN1fNo1ITU3tcT8pKUm13hl8AOAH3s38PB7p+HEpIcEs7R41SoqIMGFm/nwpLc2cE9Xaan4fS74B6wRt5eZivBO3es+tiYqK0pkzZ6xoEgCb8m7m99pr5nvvdLtTp6TMTBN4/vhHs+PwokUs+QaCQdBWbi4mOjpakvpM6m1tbWXME4DfZWRIubnmEMyUFGnCBDMcdf31JshcuCC99x6nfAPBIiQrN97hqIaGBk2ePLnzfkNDgzIzM61qFgAbu+oqU6lxuczXx4+bjftmzZK++lUzXDV+PBUbIBiEZOUmMzNTcXFxKi0t7bzX2NioyspKzZs3z8KWAbAr7/CU2y3dcIOZU+N2S598YnYvjoqiYgMEi5Cs3LhcLq1YsULr169XYmKi0tLStG7dOqWkpGjx4sVWNw+ATWVkSH/3d9Knn5pjFc6eNUvEvaeEM3kYCA4hGW4kaeXKlXK73Vq1apXOnz+vnJwcbd68OeAb+AEIb/Hx0le+YnUrAFxMSISbtWvX9rkXGRmpwsJCFRYWWtAiAAAQrEJyzg0AAMBACDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWCDcAAMBWnFY3YCT99Kc/VXFxsTwej+666y7df//9VjcJAAD4WdiEm927d+uDDz7Qq6++qtbWVt15552aP3++pk2bZnXTAACAH4XNsNTEiRP1yCOPKDIyUrGxsZo8ebLq6uqsbhYAAPCzsKncZGZmdn79zjvvqLKyUtdcc42FLQIAAIFgu3BTVFSkp59+use9G2+8UY8++qgk6fDhw/re976ntWvXKi4uzoIWAgCAQLJduFm2bJmWLVvW72N79+7Vv/7rv2r9+vW69tprR7hlAABgJNgu3Azk2LFj+sEPfqBnn31Ws2bNsro5AAAgQMIm3GzevFkXLlzQqlWrOu99//vfV35+voWtAgAA/ubweDweqxsxkE2bNqmkpERbt27tvNfR0aENGzZo27Ztamxs1Ny5c7V69Wqlp6ePWLsOHjwoj8cjp9Mph8MxYu8b6jwej9xuN/3mA/psaOg339FnQ0O/+e7ChQtyOBwBXdQTtJWbLVu26JlnnlFOTk6P+5s2bdKLL76oNWvWKDk5WevWrdNDDz2kHTt2yOVyjUjbvH+BIyLCZiW9XzgcjhH7M7IL+mxo6Dff0WdDQ7/5zuFwBDwIBl3lpr6+Xo899pgOHDiglJQUjR8/vrNy09bWpry8PBUWFuruu++WJDU2Nio/P18/+clPtHTpUiubDgAAgkDQlR4qKio0ZswYFRUVac6cOT0eq6qqUnNzs/Ly8jrvJSQkKCsrS2VlZSPdVAAAEISCbliqoKBABQUF/T7m3VE4NTW1x/2kpCTV1tYGvG0AACD4BV3l5mJaWlokqc/4ZlRUlFpbW61oEgAACDIhFW6io6Mlmbk33bW2tiomJsaKJgEAgCATUuHGOxzV0NDQ435DQ4NSUlKsaBIAAAgyIRVuMjMzFRcXp9LS0s57jY2Nqqys1Lx58yxsGQAACBZBN6H4Ylwul1asWKH169crMTFRaWlpWrdunVJSUrR48WKrmwcAAIJASIUbSVq5cqXcbrdWrVql8+fPKycnR5s3b2YTJQAAICkIN/EDAAAYjpCacwMAAHAphBsAAGArhBsAAGArhJsRUl9fP+CxEuhCP/lm/fr1Wrp0qW699Va9/vrrVjcnJHzzm9/Ubbfdpttvv12333676uvrrW5S0HvllVc6++v222/X1Vdfrc2bN1vdrKD31FNP6aabbtIdd9yh3/3ud1Y3JyQM9Bng62dDyK2WCkUlJSV6/PHHdeLECaubEtToJ9/s379fR44c0fbt23X69GndcsstWrBgQedO3uirvb1d9fX12r17txwOh9XNCRnLly/X8uXLJUmHDx/Wj370I917773WNirIFRcX66233tJvfvMbRUZG6v7771d2dnafsxHRZaDPgKF8NlC5GQEvv/yynn76aaubEfToJ9/Mnz9fzz77rCIiInTy5ElFRUUpMjLS6mYFtQ8//FCStGLFCn3961/X73//e4tbFFo6Ojr0+OOPa9WqVYToS/jggw+0aNEixcbGKioqStnZ2dq7d6/VzQpqA30GDOWzgcrNCFi/fr3VTQgJ9JPvnE6n1qxZo+eff15///d/r1GjRlndpKDW1NSk+fPn69FHH9WZM2d07733asaMGUpPT7e6aSFhz549SkxMZEf4QcjKytKTTz6pb33rW5JMpTUhIcHiVgW3gT4DhvLZQLjxk6Kioj7J8sYbb9Sjjz5qTYMQNn74wx/qu9/9rlasWKGcnBzl5uZa3aSgNW/evM4P5tGjR+uGG25QSUkJ4WaQtm3bpvvuu8/qZoSE+fPn67333tPdd9+tpKQk5ebm8o+PEUS48ZNly5Zp2bJlVjcDYeSjjz6S2+3W9OnTNXbsWOXn5+uDDz4g3FzEn//8Z0VERPSoPDid/G9wMFpbW/Xee+9pwYIFVjclJJw9e1a33nqrHn74YUnSj370I02cONHiVoUP5twAIerYsWN64okn5Ha7dfbsWe3bt0/Z2dlWNyuonTlzRk8++aQuXLigU6dO6Y033lB+fr7VzQoJ77//vqZNm0b1YZBqamr0T//0T52T2N966y1de+21VjcrbPBPFiBEFRQU6NChQ7r99tsVERGhFStWaObMmVY3K6gtXry4s886Ojr0z//8z0pOTra6WSHhs88+o698MGPGDM2fP1+33XabIiMjtXr1ao0dO9bqZoUPD/rYuHGjZ8WKFT3utbe3e/7rv/7Ls3DhQs/s2bM9DzzwgKe6utqiFgY3+s939Jnv6LOhod98R5/5Jhj6i2GpXrZs2aJnnnmmz/1NmzbpxRdf1BNPPKGXXnpJDodDDz30kNra2ixoZfCi/3xHn/mOPhsa+s139Jlvgqa/AhabQkxdXZ3nW9/6lufqq6/23HzzzT1SZ2trqyc7O9vzwgsvdN47c+aMZ/bs2Z4dO3ZY0dygQ//5jj7zHX02NPSb7+gz3wRbf1G5+YuKigqNGTNGRUVFmjNnTo/Hqqqq1NzcrLy8vM57CQkJysrKUllZ2Ug3NSjRf76jz3xHnw0N/eY7+sw3wdZfTCj+i4KCggHPrairq5OkPttmJyUlqba2NuBtCwX0n+/oM9/RZ0NDv/mOPvNNsPUXlZtBaGlpkSS5XK4e96OiotTa2mpFk0IK/ec7+sx39NnQ0G++o898Y0V/EW4GwXuGSu+JT62trYqJibGiSSGF/vMdfeY7+mxo6Dff0We+saK/CDeD4C2lNTQ09Ljf0NCglJQUK5oUUug/39FnvqPPhoZ+8x195hsr+otwMwiZmZmKi4tTaWlp573GxkZVVlZygNwg0H++o898R58NDf3mO/rMN1b0FxOKB8HlcmnFihVav369EhMTlZaWpnXr1iklJUWLFy+2unlBj/7zHX3mO/psaOg339FnvrGivwg3g7Ry5Uq53W6tWrVK58+fV05OjjZv3txnghT6R//5jj7zHX02NPSb7+gz34x0fzk8Ho8nIK8MAABgAebcAAAAWyHcAAAAWyHcAAAAWyHcAAAAWyHcAAAAWyHcAAAAWyHcAAAAWyHcAAAAWyHcAAAAWyHcAAAAW+FsKQAh580339TDDz/c+X1kZGTnIXwrV67U6NGjLWwdAKsRbgCEnIqKCknSz372MyUlJamlpUVFRUXasmWLGhsbtWbNGotbCMBKhBsAIaeyslJRUVG64YYbFBkZKUnKzc1VaWmp9uzZY3HrAFiNOTcAQk5FRYWuuOKKzmAjSREREUpMTJTT2fffbG+//bYefPBBzZ07Vzk5OXr44YdVXV09gi0GMJIINwBCypdffqnjx49r2rRpPe6fPHlSH374oW6++eYe93/2s5/pvvvuU0pKiv7zP/9TTzzxhGpra3X//ferubl5JJsOYIQwLAUgpFRWVkqSrrjiCrndbrndbr3//vv693//dy1cuFCPPPJI53P37NmjDRs2qLCwUN/+9rc770+fPl033XSTXn/9dS1btmzEfwYAgUW4ARBSvJOJn3zyST355JOd9xcsWKCnnnpKo0aN6rz3zDPPaPLkyfrbv/1bud3uzvuXX365oqOjVVNTM3INBzBiCDcAQkpFRYUiIyP1/PPPy+l06vTp0/qf//kf7du3T7/85S917733SpJOnDjRWeWZNWtWv6+VkJAwYu0GMHIINwBCSmVlpa644gplZ2d33pszZ44WLVqkX/3qV53hpra2VpL0wx/+UHPnzu33tSZPnhz4BgMYcYQbACGjqalJNTU1uuOOO3rcT0hI0OLFi/Xqq6+qpqZGkyZN0tixYyVJDodjwMoNAHsi3AAIGRUVFfJ4PJo9e3afx2666Sa9+uqr2rlzpx588EFNnjxZubm5evrpp3Xu3DnNmTNHHo9HJ06cUGlpqZYvX67c3FwLfgoAgUa4ARAyLjaHJj8/X6NHj9auXbv04IMPSpI2bdqk//7v/9Yrr7yin//854qOjlZqaqpycnI0Y8aMEW07gJHj8Hg8HqsbAQAA4C9s4gcAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGzl/wNZR77YqhH09AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Re_vec = np.logspace(0, 10, 200)\n",
    "CDs, _ = run_experiments(Re_vec=Re_vec)\n",
    "plot_CD(Re_vec, CDs, marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model_II\n",
    "class Model_II(nn.Module):\n",
    "    def __init__(self, input_dim=4 , output_dim= 1, hidden_dims=[16, 8, 4]):\n",
    "        super(Model_II, self).__init__()\n",
    "        hidden_dims.insert(0, input_dim)\n",
    "        hidden_dims.append(output_dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_dims[i], hidden_dims[i+1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = torch.relu(self.layers[i](x))\n",
    "        x = self.layers[-1](x)  # Apply the last layer without ReLU\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generation:\n",
    "# Let's first generate random samples of rho, mu, D, and U. The true label is then calculated using the run_experiments function. \n",
    "# The Reynolds numbers are also calculated and returned as a vector for convenience.\n",
    "def sample_parameters(num_samples=5000, rho_range=[100, 2000], mu_range=[0.001, 0.01], D_range=[0.05, 0.5], U_range=[0.1, 20], seed=123):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    rho = np.random.uniform(rho_range[0], rho_range[1], num_samples)\n",
    "    mu = np.random.uniform(mu_range[0], mu_range[1], num_samples)\n",
    "    D = np.random.uniform(D_range[0], D_range[1], num_samples)\n",
    "    U = np.random.uniform(U_range[0], U_range[1], num_samples)\n",
    "\n",
    "    # Calculate the true CD and FD:\n",
    "    CD, _ = run_experiments(Re_vec=None, velocities=U, diameters=D, densities=rho, viscosities=mu)\n",
    "    Re_vec = rho * U * D / mu\n",
    "    return rho, mu, D, U, Re_vec, CD\n",
    "\n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, num_samples=5000, rho_range=[100, 2000], mu_range=[0.001, 0.01], D_range=[0.05, 0.5], U_range=[0.1, 20], seed=None):\n",
    "        self.rho, self.mu, self.D, self.U, self.Re, self.CD = sample_parameters(num_samples=num_samples, rho_range=rho_range, mu_range=mu_range, D_range=D_range, U_range=U_range, seed=seed)\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sample = np.array([self.rho[idx], self.mu[idx], self.D[idx], self.U[idx]], dtype=np.float32)\n",
    "        target = np.array([self.CD[idx]], dtype=np.float32)\n",
    "        return input_sample, target\n",
    "\n",
    "\n",
    "# Create the dataloader/dataset:\n",
    "dataset = RandomDataset(num_samples=5000, rho_range=[100, 2000], mu_range=[0.001, 0.01], D_range=[0.05, 0.5], U_range=[0.1, 20], seed=123)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets:\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0) #TODO: Change num_workers to 4. Need to take RandomDataset out of the main function\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 290.0896, Validation Loss: 2.9396\n",
      "Epoch 2/100, Train Loss: 0.5932, Validation Loss: 0.1240\n",
      "Epoch 3/100, Train Loss: 0.1077, Validation Loss: 0.1024\n",
      "Epoch 4/100, Train Loss: 0.1015, Validation Loss: 0.0998\n",
      "Epoch 5/100, Train Loss: 0.0985, Validation Loss: 0.0966\n",
      "Epoch 6/100, Train Loss: 0.0952, Validation Loss: 0.0933\n",
      "Epoch 7/100, Train Loss: 0.0918, Validation Loss: 0.0899\n",
      "Epoch 8/100, Train Loss: 0.0883, Validation Loss: 0.0866\n",
      "Epoch 9/100, Train Loss: 0.0847, Validation Loss: 0.0832\n",
      "Epoch 10/100, Train Loss: 0.0811, Validation Loss: 0.0795\n",
      "Epoch 11/100, Train Loss: 0.0776, Validation Loss: 0.0766\n",
      "Epoch 12/100, Train Loss: 0.0740, Validation Loss: 0.0727\n",
      "Epoch 13/100, Train Loss: 0.0707, Validation Loss: 0.0695\n",
      "Epoch 14/100, Train Loss: 0.0673, Validation Loss: 0.0663\n",
      "Epoch 15/100, Train Loss: 0.0642, Validation Loss: 0.0633\n",
      "Epoch 16/100, Train Loss: 0.0611, Validation Loss: 0.0605\n",
      "Epoch 17/100, Train Loss: 0.0582, Validation Loss: 0.0576\n",
      "Epoch 18/100, Train Loss: 0.0556, Validation Loss: 0.0552\n",
      "Epoch 19/100, Train Loss: 0.0532, Validation Loss: 0.0534\n",
      "Epoch 20/100, Train Loss: 0.0508, Validation Loss: 0.0505\n",
      "Epoch 21/100, Train Loss: 0.0483, Validation Loss: 0.0486\n",
      "Epoch 22/100, Train Loss: 0.0464, Validation Loss: 0.0464\n",
      "Epoch 23/100, Train Loss: 0.0443, Validation Loss: 0.0447\n",
      "Epoch 24/100, Train Loss: 0.0426, Validation Loss: 0.0434\n",
      "Epoch 25/100, Train Loss: 0.0412, Validation Loss: 0.0416\n",
      "Epoch 26/100, Train Loss: 0.0399, Validation Loss: 0.0400\n",
      "Epoch 27/100, Train Loss: 0.0383, Validation Loss: 0.0387\n",
      "Epoch 28/100, Train Loss: 0.0370, Validation Loss: 0.0380\n",
      "Epoch 29/100, Train Loss: 0.0357, Validation Loss: 0.0363\n",
      "Epoch 30/100, Train Loss: 0.0346, Validation Loss: 0.0358\n",
      "Epoch 31/100, Train Loss: 0.0338, Validation Loss: 0.0356\n",
      "Epoch 32/100, Train Loss: 0.0329, Validation Loss: 0.0340\n",
      "Epoch 33/100, Train Loss: 0.0319, Validation Loss: 0.0327\n",
      "Epoch 34/100, Train Loss: 0.0312, Validation Loss: 0.0319\n",
      "Epoch 35/100, Train Loss: 0.0307, Validation Loss: 0.0311\n",
      "Epoch 36/100, Train Loss: 0.0297, Validation Loss: 0.0305\n",
      "Epoch 37/100, Train Loss: 0.0290, Validation Loss: 0.0298\n",
      "Epoch 38/100, Train Loss: 0.0284, Validation Loss: 0.0296\n",
      "Epoch 39/100, Train Loss: 0.0282, Validation Loss: 0.0297\n",
      "Epoch 40/100, Train Loss: 0.0274, Validation Loss: 0.0280\n",
      "Epoch 41/100, Train Loss: 0.0268, Validation Loss: 0.0273\n",
      "Epoch 42/100, Train Loss: 0.0265, Validation Loss: 0.0276\n",
      "Epoch 43/100, Train Loss: 0.0260, Validation Loss: 0.0262\n",
      "Epoch 44/100, Train Loss: 0.0252, Validation Loss: 0.0286\n",
      "Epoch 45/100, Train Loss: 0.0248, Validation Loss: 0.0257\n",
      "Epoch 46/100, Train Loss: 0.0243, Validation Loss: 0.0246\n",
      "Epoch 47/100, Train Loss: 0.0237, Validation Loss: 0.0240\n",
      "Epoch 48/100, Train Loss: 0.0234, Validation Loss: 0.0235\n",
      "Epoch 49/100, Train Loss: 0.0228, Validation Loss: 0.0238\n",
      "Epoch 50/100, Train Loss: 0.0224, Validation Loss: 0.0225\n",
      "Epoch 51/100, Train Loss: 0.0217, Validation Loss: 0.0222\n",
      "Epoch 52/100, Train Loss: 0.0213, Validation Loss: 0.0218\n",
      "Epoch 53/100, Train Loss: 0.0212, Validation Loss: 0.0211\n",
      "Epoch 54/100, Train Loss: 0.0206, Validation Loss: 0.0207\n",
      "Epoch 55/100, Train Loss: 0.0204, Validation Loss: 0.0209\n",
      "Epoch 56/100, Train Loss: 0.0199, Validation Loss: 0.0209\n",
      "Epoch 57/100, Train Loss: 0.0196, Validation Loss: 0.0195\n",
      "Epoch 58/100, Train Loss: 0.0191, Validation Loss: 0.0190\n",
      "Epoch 59/100, Train Loss: 0.0188, Validation Loss: 0.0186\n",
      "Epoch 60/100, Train Loss: 0.0185, Validation Loss: 0.0188\n",
      "Epoch 61/100, Train Loss: 0.0182, Validation Loss: 0.0178\n",
      "Epoch 62/100, Train Loss: 0.0177, Validation Loss: 0.0175\n",
      "Epoch 63/100, Train Loss: 0.0174, Validation Loss: 0.0174\n",
      "Epoch 64/100, Train Loss: 0.0171, Validation Loss: 0.0177\n",
      "Epoch 65/100, Train Loss: 0.0169, Validation Loss: 0.0166\n",
      "Epoch 66/100, Train Loss: 0.0164, Validation Loss: 0.0177\n",
      "Epoch 67/100, Train Loss: 0.0165, Validation Loss: 0.0160\n",
      "Epoch 68/100, Train Loss: 0.0162, Validation Loss: 0.0157\n",
      "Epoch 69/100, Train Loss: 0.0158, Validation Loss: 0.0162\n",
      "Epoch 70/100, Train Loss: 0.0156, Validation Loss: 0.0156\n",
      "Epoch 71/100, Train Loss: 0.0153, Validation Loss: 0.0150\n",
      "Epoch 72/100, Train Loss: 0.0151, Validation Loss: 0.0152\n",
      "Epoch 73/100, Train Loss: 0.0150, Validation Loss: 0.0148\n",
      "Epoch 74/100, Train Loss: 0.0158, Validation Loss: 0.0144\n",
      "Epoch 75/100, Train Loss: 0.0152, Validation Loss: 0.0143\n",
      "Epoch 76/100, Train Loss: 0.0146, Validation Loss: 0.0140\n",
      "Epoch 77/100, Train Loss: 0.0148, Validation Loss: 0.0148\n",
      "Epoch 78/100, Train Loss: 0.0144, Validation Loss: 0.0149\n",
      "Epoch 79/100, Train Loss: 0.0151, Validation Loss: 0.0136\n",
      "Epoch 80/100, Train Loss: 0.0140, Validation Loss: 0.0135\n",
      "Epoch 81/100, Train Loss: 0.0151, Validation Loss: 0.0133\n",
      "Epoch 82/100, Train Loss: 0.0148, Validation Loss: 0.0138\n",
      "Epoch 83/100, Train Loss: 0.0143, Validation Loss: 0.0145\n",
      "Epoch 84/100, Train Loss: 0.0136, Validation Loss: 0.0165\n",
      "Epoch 85/100, Train Loss: 0.0155, Validation Loss: 0.0133\n",
      "Epoch 86/100, Train Loss: 0.0137, Validation Loss: 0.0140\n",
      "Epoch 87/100, Train Loss: 0.0136, Validation Loss: 0.0147\n",
      "Epoch 88/100, Train Loss: 0.0138, Validation Loss: 0.0131\n",
      "Epoch 89/100, Train Loss: 0.0143, Validation Loss: 0.0167\n",
      "Epoch 90/100, Train Loss: 0.0142, Validation Loss: 0.0136\n",
      "Epoch 91/100, Train Loss: 0.0146, Validation Loss: 0.0134\n",
      "Epoch 92/100, Train Loss: 0.0136, Validation Loss: 0.0128\n",
      "Epoch 93/100, Train Loss: 0.0134, Validation Loss: 0.0124\n",
      "Epoch 94/100, Train Loss: 0.0141, Validation Loss: 0.0150\n",
      "Epoch 95/100, Train Loss: 0.0141, Validation Loss: 0.0154\n",
      "Epoch 96/100, Train Loss: 0.0146, Validation Loss: 0.0129\n",
      "Epoch 97/100, Train Loss: 0.0168, Validation Loss: 0.0129\n",
      "Epoch 98/100, Train Loss: 0.0134, Validation Loss: 0.0123\n",
      "Epoch 99/100, Train Loss: 0.0165, Validation Loss: 0.0154\n",
      "Epoch 100/100, Train Loss: 0.0150, Validation Loss: 0.0122\n",
      "Final Test Loss: 0.0123\n"
     ]
    }
   ],
   "source": [
    "model = Model_II(input_dim=4, output_dim=1, hidden_dims=[16, 8, 4])\n",
    "model = model.float()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "writer = SummaryWriter('runs/Model_II')\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Used to save the best model:\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in train_loader: #tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} [Training]'):\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader: #tqdm(val_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} [Validation]'):\n",
    "            inputs, targets = inputs.float(), targets.float()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_state = model.state_dict().copy()\n",
    "\n",
    "    tqdm.write(f'Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "# Test loop\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "writer.add_scalar('Loss/Test', test_loss, NUM_EPOCHS)\n",
    "\n",
    "tqdm.write(f'Final Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Save the best model:\n",
    "torch.save(best_model_state, 'models/Model-II-checkpoint.pth')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference & Testing:  \n",
    "Visualize the model fit by looking at the curves for a regularly-spaced Re vector covering the entire range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments:\n",
    "Generating some 'experimental' data and comparing different $\\Pi$ groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Material parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take water as an example: rho = 1000Kg/m^3, mu = 0.001 Ns/M^2\n",
    "# c_sound (Cs) in water ~ 1481 m/s, Cs(Acetone) = ~1100 m/s - We need to make sure the velocity is < 0.3*Cs\n",
    "N = 200 # Number of samples\n",
    "Cs = 1481 # m/s\n",
    "u_max = 0.3 * Cs # Picked 0.2 to make sure the condition is not violated for acetone\n",
    "\n",
    "mu_water = 0.001 # [SI]\n",
    "rho_water = 1000 # [SI]\n",
    "\n",
    "mu_acetone = 0.000316 # Acetone [SI]\n",
    "rho_acetone = 784.5 # Acetone [SI]\n",
    "\n",
    "# Sphere diameters:\n",
    "D_I = 0.05 # Diameter of sphere\n",
    "D_II = 0.1\n",
    "D_III = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the 'Experiments':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's keep the velocities the same for now:\n",
    "us = np.geomspace(0.001, u_max, N)\n",
    "\n",
    "# Experiment #1:\n",
    "mu_1 = mu_water\n",
    "rho_1 = rho_water\n",
    "D_1 = D_I\n",
    "REs_1 = [rho_1*uu*D_1/mu_1 for uu in us]\n",
    "CDs_1, FDs_1 = run_experiment(velocities=us, diameter=D_1, density=rho_1, viscosity=mu_1)\n",
    "fig, ax1, ax2 = make_plots(Re=REs_1, CD=CDs_1, FD=FDs_1, experiment_id='Experiment-1', colors=['blue', 'blue'])\n",
    "\n",
    "# Experiment #2:\n",
    "mu_2 = mu_acetone\n",
    "rho_2 = rho_acetone\n",
    "D_2 = D_I\n",
    "REs_2 = [rho_2*uu*D_2/mu_2 for uu in us]\n",
    "CDs_2, FDs_2 = run_experiment(velocities=us, diameter=D_2, density=rho_2, viscosity=mu_2)\n",
    "fig, ax1, ax2 = make_plots(Re=REs_2, CD=CDs_2, FD=FDs_2, experiment_id='Experiment-1 and 2', colors=['red', 'red'], fig=fig, ax1=ax1, ax2=ax2)\n",
    "\n",
    "# Experiment #3:\n",
    "mu_3 = mu_acetone\n",
    "rho_3 = rho_acetone\n",
    "D_3 = D_III\n",
    "REs_3 = [rho_3*uu*D_3/mu_3 for uu in us]\n",
    "CDs_3, FDs_3 = run_experiment(velocities=us, diameter=D_3, density=rho_3, viscosity=mu_3)\n",
    "fig, ax1, ax2 = make_plots(Re=REs_3, CD=CDs_3, FD=FDs_3, experiment_id='Experiment-3', colors=['green', 'green'], fig=fig, ax1=ax1, ax2=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing to other $\\Pi$ groups:\n",
    "It appears $\\Pi_4$ does not work but the rest do! Why??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Comparing the following Pi group:\n",
    "\n",
    "# Experiment #1:\n",
    "mu_1 = mu_water\n",
    "rho_1 = rho_water\n",
    "D_1 = D_I\n",
    "REs_1 = [rho_1*uu*D_1/mu_1 for uu in us]\n",
    "CDs_1, FDs_1 = run_experiment(velocities=us, diameter=D_1, density=rho_1, viscosity=mu_1)\n",
    "# fig, ax1, ax2 = make_plots(Re=REs_1, CD=CDs_1, FD=FDs_1, experiment_id='Experiment-1', colors=['blue', 'blue'])\n",
    "\n",
    "pi2_1 = [FDs/(mu_1*D_1*uu) for (FDs, uu) in zip(FDs_1, us)]\n",
    "pi3_1 = [FDs*rho_1/mu_1**2 for FDs in FDs_1]\n",
    "\n",
    "fig, ax1, ax2 = make_plots(Re=REs_1, CD=pi2_1, FD=FDs_1, experiment_id='Experiment-1', \n",
    "                           colors=['blue', 'blue'], ylabel='$\\Pi_2$')\n",
    "\n",
    "# Experiment #2:\n",
    "mu_2 = mu_acetone\n",
    "rho_2 = rho_acetone\n",
    "D_2 = D_I\n",
    "REs_2 = [rho_2*uu*D_2/mu_2 for uu in us]\n",
    "CDs_2, FDs_2 = run_experiment(velocities=us, diameter=D_2, density=rho_2, viscosity=mu_2)\n",
    "\n",
    "pi2_2 = [FDs/(mu_2*D_2*uu) for (FDs, uu) in zip(FDs_2, us)]\n",
    "pi3_2 = [FDs*rho_2/mu_2**2 for FDs in FDs_2]\n",
    "\n",
    "fig, ax1, ax2 = make_plots(Re=REs_2, CD=pi2_2, FD=FDs_2, experiment_id='Experiment-1 and 2', \n",
    "                           colors=['red', 'red'], fig=fig, ax1=ax1, ax2=ax2, ylabel='$\\Pi_2$')\n",
    "\n",
    "# Experiment #3:\n",
    "mu_3 = mu_acetone\n",
    "rho_3 = rho_acetone\n",
    "D_3 = D_III\n",
    "REs_3 = [rho_3*uu*D_3/mu_3 for uu in us]\n",
    "CDs_3, FDs_3 = run_experiment(velocities=us, diameter=D_3, density=rho_3, viscosity=mu_3)\n",
    "\n",
    "pi2_3 = [FDs/(mu_3*D_3*uu) for (FDs, uu) in zip(FDs_3, us)]\n",
    "pi3_3 = [FDs*rho_3/mu_3**2 for FDs in FDs_3]\n",
    "\n",
    "fig, ax1, ax2 = make_plots(Re=REs_3, CD=pi2_3, FD=FDs_3, experiment_id='Experiment-3', \n",
    "                           colors=['green', 'green'], fig=fig, ax1=ax1, ax2=ax2, ylabel='$\\Pi_2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more ideas discussed with Hamed (December 27th, 2020):\n",
    "* Can we somehow use different units (e.g., SI vs imperial) to allow the network to learn the invariance to change of units?\n",
    "* Explore online data collection...depending on the results of a given experiment, the network selects what data point is needed\n",
    "* Can we use the knowledge of the number of possible dimensionless groups to create an appropriate architecture? For example, one simple way in this case is to use two nodes in the penultimate (or else) layer since we expect a simpler relationship between the two $\\Pi$ groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Regression\n",
    "Method #1:  \n",
    "$F_D$ = $f(\\rho, \\mu, U, D)$\n",
    "\n",
    "Method #2:  \n",
    "$F_D$ = $f(Re, \\rho, U, D)$\n",
    "\n",
    "Method #3:  \n",
    "$C_D$ = $f(Re, \\rho, U, D)$\n",
    "\n",
    "For all three methods, we use a deep neural network to approximate the function $f$.\n",
    "\n",
    "Method #4:\n",
    "Now, let's say we somehow know that there are 2 dimensionless parameters. 'Design' the architecture (i.e., bottleneck), so that there are 2 bottleneck nodes before we estimate the $F_D$. Compare the values of the bottleneck nodes to the $Re$ and $C_D$. Is there a correspondence? if not, how can we nudge them to be??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following if you want to re-generate the dataset. It takes a couple of minutes.\n",
    "\n",
    "# # Picking some ranges for the parameters:\n",
    "# N = 50 # Number of points\n",
    "# mu_vec = np.linspace(0.0001, 0.01, N)\n",
    "# rho_vec = np.linspace(500, 2000, N)\n",
    "# U_vec = np.linspace(0.001, 200, 2*N)\n",
    "# D_vec = np.linspace(0.01, 0.1, N)\n",
    "\n",
    "# FDs = np.zeros((N, N, 2*N, N))\n",
    "# CDs = np.zeros((N, N, 2*N, N))\n",
    "# counter = 0\n",
    "# X, Y = [], []\n",
    "# for ii, m in enumerate(mu_vec):\n",
    "#     for jj, r in enumerate(rho_vec):\n",
    "#         for kk, u in enumerate(U_vec):\n",
    "#             for ll, d in enumerate(D_vec):\n",
    "#                 CDs[ii, jj, kk, ll], FDs[ii, jj, kk, ll] = CD_calc(Re=None, U=u, D=d, rho=r, mu=m)\n",
    "#                 X.append([m, r, u, d])\n",
    "#                 Y.append(FDs[ii, jj, kk, ll])\n",
    "                \n",
    "# X = np.array(X)\n",
    "# Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the generated data in the previous cell:\n",
    "# with open('dataset1.npy', 'wb') as f:\n",
    "#     np.save(f, mu_vec)\n",
    "#     np.save(f, rho_vec)\n",
    "#     np.save(f, U_vec)\n",
    "#     np.save(f, D_vec)\n",
    "#     np.save(f, CDs)\n",
    "#     np.save(f, FDs)\n",
    "#     np.save(f, X)\n",
    "#     np.save(f, Y)\n",
    "    \n",
    "# Loading the saved generated data:\n",
    "with open('dataset1.npy', 'rb') as f:\n",
    "    mu_vec = np.load(f)\n",
    "    rho_vec = np.load(f)\n",
    "    U_vec = np.load(f)\n",
    "    D_vec = np.load(f)\n",
    "    CDs = np.load(f)\n",
    "    FDs = np.load(f)\n",
    "    X = np.load(f)\n",
    "    Y = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.max(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale input variables so that they are roughly in the same range:\n",
    "X[:,0] = X[:,0] * 1e4\n",
    "X[:,1] = X[:,1] * 1e-2\n",
    "X[:,2] = X[:,2] * 1e-1\n",
    "X[:,3] = X[:,3] * 1e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# convert to tensors and then Variables\n",
    "x_data, y_data = Variable(torch.tensor(X)), Variable(torch.tensor(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD #1:\n",
    "# Dimensions of hidden units\n",
    "D_in = 4 # mu, rho, U, D\n",
    "H = [32, 16, 8] # 3 Hidden units\n",
    "D_out = 1 # Drag force\n",
    "model_1 = torch.nn.Sequential(torch.nn.Linear(D_in, H[0]), \n",
    "                              torch.nn.BatchNorm2d(H[0]),\n",
    "                              torch.nn.ReLU(), \n",
    "                              torch.nn.Linear(H[0], H[1]),\n",
    "                              torch.nn.BatchNorm2d(H[1]),\n",
    "                              torch.nn.ReLU(),\n",
    "                              torch.nn.Linear(H[1], H[2]),\n",
    "                              torch.nn.BatchNorm2d(H[2]),\n",
    "                              torch.nn.ReLU(), \n",
    "                              torch.nn.Linear(H[2], D_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test running the model\n",
    "x_test = X[0,:].astype('float32')\n",
    "print(x_test)\n",
    "\n",
    "x_test_torch = torch.tensor(x_test)\n",
    "out_test = model_1(x_test_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hpyerparameters:\n",
    "N = len(x_data) # number of training samples\n",
    "learning_rate = 1e-5\n",
    "batch_size = 100000\n",
    "batches_per_epoch = N // batch_size\n",
    "epochs = 1\n",
    "max_batches = epochs * batches_per_epoch\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model_1.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training:\n",
    "print(\"Start Training...\")\n",
    "\n",
    "for b in range(max_batches):\n",
    "    curr_batch = np.random.choice(N, batch_size, replace=False)\n",
    "    xx = torch.tensor(X[curr_batch].astype('float32'))\n",
    "    yy = torch.tensor(Y[curr_batch].astype('float32')).view(batch_size, 1)\n",
    "    \n",
    "    # zero the gradients:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Predict:\n",
    "    y_pred = model_1(xx)\n",
    "    \n",
    "    # Calculate loss:\n",
    "    loss = loss_fn(y_pred, yy)\n",
    "    \n",
    "    # Calculate gradients:\n",
    "    loss.backward()\n",
    "    \n",
    "    # Step:\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print out some metrics:\n",
    "    if b % (max_batches // 10) == 0:\n",
    "        print(\"batch = %6d\" % b, end=\"\")\n",
    "        print(\"  batch loss = %7.4f\" % loss.item(), end=\"\")\n",
    "        \n",
    "print(\"Training complete \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Figure out why NaNs are produced at the output....probably related to scaling of the variables. Use batchnorm?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
